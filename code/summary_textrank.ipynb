{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding  on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking?\n",
      "Tests conducted by the FAA use planes with a  31 inch pitch, a standard which on some airlines has decreased .\n",
      "But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.\n",
      "\n",
      "Extracted Keywords:\n",
      "['inch', 'inches', 'seat', 'plane seats', 'airlines', 'planes', 'air', 'humans', 'humane', 'human', 'said', 'consumer', 'aviation', 'tests', 'lockers crashing', 'offer', 'offers', 'set']\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "from summa import keywords\n",
    "\n",
    "def text_rank_summarize(text, ratio=0.2):\n",
    "    summarized_text = summarizer.summarize(text, ratio=ratio)\n",
    "    return summarized_text\n",
    "\n",
    "def text_rank_extract_keywords(text, ratio=0.2):\n",
    "    extracted_keywords = keywords.keywords(text, ratio=ratio).split('\\n')\n",
    "    return extracted_keywords\n",
    "\n",
    "# input from the user without gui\n",
    "input_text = input(\"Enter your input text: \")\n",
    "\n",
    "# Summarization\n",
    "summary = text_rank_summarize(input_text)\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "print()\n",
    "\n",
    "# Keyword extraction\n",
    "keywords = text_rank_extract_keywords(input_text)\n",
    "print(\"Extracted Keywords:\")\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def get_cosine_similarity(text1, text2):\n",
    "    # Preprocess texts by converting them to lowercase and removing non-alphanumeric characters\n",
    "    text1 = re.sub(r'\\W+', ' ', text1.lower())\n",
    "    text2 = re.sub(r'\\W+', ' ', text2.lower())\n",
    "\n",
    "    # Tokenize texts into words\n",
    "    words1 = text1.split()\n",
    "    words2 = text2.split()\n",
    "\n",
    "    # Count word frequencies in each text\n",
    "    word_freq1 = Counter(words1)\n",
    "    word_freq2 = Counter(words2)\n",
    "\n",
    "    # Get the set of all unique words\n",
    "    all_words = set(words1).union(set(words2))\n",
    "\n",
    "    # Calculate the dot product of word frequencies\n",
    "    dot_product = sum(word_freq1[word] * word_freq2[word] for word in all_words)\n",
    "\n",
    "    # Calculate the Euclidean lengths of word frequencies\n",
    "    length1 = math.sqrt(sum(word_freq1[word] ** 2 for word in all_words))\n",
    "    length2 = math.sqrt(sum(word_freq2[word] ** 2 for word in all_words))\n",
    "\n",
    "    # Calculate the cosine similarity score\n",
    "    cosine_similarity = dot_product / (length1 * length2)\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "# Example usage\n",
    "text1 = \"This is the first text.\"\n",
    "text2 = \"This is the second text.\"\n",
    "\n",
    "similarity_score = get_cosine_similarity(text1, text2)\n",
    "print(\"Similarity score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.7.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pycountry>=18.2.23\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docopt<0.7,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Collecting breadability>=0.1.20\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "Collecting nltk>=3.0.2\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet\n",
      "  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 32.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lxml>=2.0\n",
      "  Downloading lxml-4.9.2-cp38-cp38-macosx_10_15_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 58.9 MB/s eta 0:00:01�███████████████████▏     | 3.9 MB 58.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 62.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.6.3-cp38-cp38-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 59.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/bgrnaymane/Documents/GitHub/Projektrealisierung_Gruppe5/.venv/lib/python3.8/site-packages (from pycountry>=18.2.23->sumy) (56.0.0)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 65.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-macosx_10_9_x86_64.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 20.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 38.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 421 kB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21684 sha256=7589789488032838391f3a6b36574b294a1cccbb69d89895000745e0f91636f4\n",
      "  Stored in directory: /Users/bgrnaymane/Library/Caches/pip/wheels/5f/0d/0c/2062d8c1758b4b1a2e42b4a63e6660d9ec2ba9463cfee9eeab\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=e1a02bcd24f3830cf756d166515a3f93c33ec2cc60a8a7f7731a0e7bf024c698\n",
      "  Stored in directory: /Users/bgrnaymane/Library/Caches/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "  Building wheel for pycountry (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=2413ca4c26392202af121579c215ae3857993d0c9320747028d942011cf03773\n",
      "  Stored in directory: /Users/bgrnaymane/Library/Caches/pip/wheels/e2/aa/0f/c224e473b464387170b83ca7c66947b4a7e33e8d903a679748\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: urllib3, tqdm, regex, lxml, joblib, idna, docopt, click, charset-normalizer, chardet, certifi, requests, pycountry, nltk, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 certifi-2023.5.7 chardet-5.1.0 charset-normalizer-3.1.0 click-8.1.3 docopt-0.6.2 idna-3.4 joblib-1.2.0 lxml-4.9.2 nltk-3.8.1 pycountry-22.3.5 regex-2023.6.3 requests-2.31.0 sumy-0.11.0 tqdm-4.65.0 urllib3-2.0.3\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/bgrnaymane/Documents/GitHub/Projektrealisierung_Gruppe5/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bgrnaymane/Documents/GitHub/Projektrealisierung_Gruppe5/.venv/lib/python3.8/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataframe\n",
    "df = pd.read_csv('../data/BBC_News_Summary/')\n",
    "\n",
    "# Show Dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the summarizers\n",
    "text_rank_summarizer = TextRankSummarizer()\n",
    "lsa_summarizer = LsaSummarizer()\n",
    "lex_rank_summarizer = LexRankSummarizer()\n",
    "\n",
    "# Define the compression rates\n",
    "compression_rates = [0.1, 0.3, 0.5, 0.7]  # Adjust the values as desired\n",
    "\n",
    "# Initialize counter\n",
    "approach_counts = {summarizer.__class__.__name__: 0 for summarizer in [text_rank_summarizer, lsa_summarizer, lex_rank_summarizer]}\n",
    "compression_rate_counts = {rate: 0 for rate in compression_rates}\n",
    "\n",
    "# Iterate over dataframe\n",
    "for index, row in df.iterrows():\n",
    "    approach = random.choice([text_rank_summarizer, lsa_summarizer, lex_rank_summarizer])\n",
    "    compression_rate = random.choice(compression_rates)\n",
    "    \n",
    "    # Update counter\n",
    "    approach_counts[approach.__class__.__name__] += 1\n",
    "    compression_rate_counts[compression_rate] += 1\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(\"english\")\n",
    "    sentences = tokenizer.to_sentences(row['Text'])\n",
    "\n",
    "    # Calculate the number of sentences for the chosen compression rate\n",
    "    num_sentences = int(len(sentences) * compression_rate)\n",
    "\n",
    "    # Generate the summary\n",
    "    summarizer = approach\n",
    "    parser = PlaintextParser.from_string(row['Text'], tokenizer)\n",
    "    summary = summarizer(parser.document, num_sentences)\n",
    "\n",
    "    # Store the summary in the \"Summary\" column\n",
    "    df.loc[index, 'Summary'] = ' '.join(str(sentence) for sentence in summary)\n",
    "\n",
    "# Calculate total summaries\n",
    "total_summaries = len(df)\n",
    "\n",
    "# Calculate the percentages\n",
    "approach_percentages = {approach: (count / total_summaries) * 100 for approach, count in approach_counts.items()}\n",
    "compression_rate_percentages = {rate: (count / total_summaries) * 100 for rate, count in compression_rate_counts.items()}\n",
    "\n",
    "# Print the results\n",
    "for approach, percentage in approach_percentages.items():\n",
    "    print(f\"The approach {approach} was used for {percentage:.2f}% of the summaries.\")\n",
    "\n",
    "for rate, percentage in compression_rate_percentages.items():\n",
    "    print(f\"The compression rate {rate} was used for {percentage:.2f}% of the summaries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
