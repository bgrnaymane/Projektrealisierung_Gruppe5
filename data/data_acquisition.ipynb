{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presidential Speeches - Data\n",
    "Data Source: https://millercenter.org/presidential-speeches-downloadable-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data saved to ./data/Speeches/presidential-speeches.json successfully.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://millercenter.org/sites/default/files/corpus/presidential-speeches.json\"\n",
    "filename = \"./data/Speeches/presidential-speeches.json\"\n",
    "\n",
    "# Get request to the url\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the JSON data from the response\n",
    "    json_data = response.json()\n",
    "\n",
    "    # Save the JSON data to a file\n",
    "    with open(filename, \"w\") as file:\n",
    "        json.dump(json_data, file)\n",
    "\n",
    "    print(f\"JSON data saved to {filename} successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "801\n"
     ]
    }
   ],
   "source": [
    "with open('../data/Speeches/presidential-speeches.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "speech_texts = [element[\"transcript\"] for element in data if len(element[\"transcript\"].split())>=1100]\n",
    "print(len(speech_texts))\n",
    "for i in range(len(speech_texts)):\n",
    "    speech_texts[i] = speech_texts[i].replace(\"\\r\\n\\r\\n\", \" \")\n",
    "    speech_texts[i] = speech_texts[i].replace(\"\\r\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4915.5830212234705\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for transcript in speech_texts:\n",
    "    words = transcript.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_texts_short = []\n",
    "\n",
    "for text in speech_texts:\n",
    "    target_word_count = random.randint(500, 800)\n",
    "    total_word_count = 0\n",
    "    selected_sentences = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    sentence_length = []\n",
    "    for sentence in sentences:\n",
    "        sentence_length.append(len(sentence.split()))\n",
    "\n",
    "    max_skip = (len(sentences)*int(mean(sentence_length)))-target_word_count\n",
    "    words_to_skip = random.randint(0, max_skip)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        if total_word_count + len(words) <= words_to_skip:\n",
    "            total_word_count += len(words)\n",
    "            continue\n",
    "        if total_word_count + len(words) <= target_word_count + words_to_skip:\n",
    "            selected_sentences.append(sentence)\n",
    "            total_word_count += len(words)\n",
    "        else:\n",
    "            break\n",
    "    speech_texts_short.append(' '.join(selected_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_class = ['Political speech'] * len(speech_texts_short)\n",
    "\n",
    "df_speeches = pd.DataFrame({'Text': speech_texts_short, 'Class': speech_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have heard nothing from the Ambassador about any intention to leave. I have every reason to believe that if he had any plans, he would make them known. I fully covered, in my conference last week, my views toward the Ambassador's service, and I believe when and if he has any plans to leave the State Department service, he will communicate them to me. Q. Mr. President, in your letter to Soviet Premier Khrushchev on Wednesday regarding Cyprus you mentioned basic misunderstandings. Because of this misunderstanding and others, would a personal meeting between you and Khrushchev be desirable at this point? THE PRESIDENT. I think that we are in adequate communication with each other. I would be very happy to see the Chairman when it is indicated that there are any things that we can explore that would be helpful. I know of no reason for such a meeting at this time. Q. Mr. President, in answering an earlier question about the Soviet trade overture, did you mean to imply that trade between the Soviet Union and the United States should be on an individual item basis in the mutual interest of the two countries, or were you opening the possibility of a trade agreement between the Soviet Union and the U.S., such as the Russians have with some of the Western countries? THE PRESIDENT. The answer is \"No\" to both of your questions. Q. Mr. President, in connection with your announcement concerning various diseases, since the U.S. Public Health Service has so strongly condemned the use of tobacco as a health hazard, do you see any justification at all for continued Government subsidy to tobacco growers? THE PRESIDENT. I don't think that the report has been made a Government report as yet. I understand this committee was appointed by the Surgeon General with the understanding that when they made their recommendations, that report would be submitted to all the departments of Government concerned, and that would be the second procedure followed. They, in turn, would carefully digest and study its recommendations and then make the recommendations back to the Secretary of Health, Education, and Welfare. The Government agencies concerned are now making that study and in due time will make their recommendations. Q. Mr. President, I believe you said early in your administration that you were not considering any trips overseas before election time. Has there been any change in your thinking on that? THE PRESIDENT. No. Q. Mr. President, you said earlier that you had been in communication with President de Gaulle. Without asking you, sir, for any of the details of those private communications, could you say, sir, whether the United States and France have exchanged general views about their policies in Southeast Asia? THE PRESIDENT. I am aware of no detailed plan that General de Gaulle has concerning Southeast Asia. Our Government has discussed with representatives of his government certain phases of that situation, but so far as I am personally aware I know of no specific detailed plan that the General may have advanced. Q. Mr. President, in talking to a group of senior citizens about medicare, you made this statement: \"We are going to try to take all of the money that we think is unnecessarily being spent and take it from the haves and give it to the have-nots that need it so much.\" I just wondered if you could elaborate, sir. THE PRESIDENT. I think that explains itself. We have taken about $3 billion out of the budget as constituted last year, 98.8. We reduced that budget by about $3 billion, by cutting $1,100 million out of Defense, almost $r billion out of Agriculture, and almost $100 million out of the Post Office, 150 out of Atomic Energy, and so forth. We reduced it $3 billion. Now we thought that all of those reductions could be made. They had appropriations for them last year. We are not asking for appropriations for them this year. So we will save $3 billion there. But we are asking for an additional $2 billion to be put in the budget. Roughly, that is $400 million extra interest rate on the public debt, $600 million for space. That is a billion. Then we have the poverty program and the Appalachia program, roughly a half-million dollars, $300 million extra for education, 75 for urban renewal, 75 for public housing, and we expect those programs to have money this year taken from those programs that we did not ask for money that they had last year. We expect the total budget to be a little less than a billion dollars less than the Kennedy budget of last year.\n"
     ]
    }
   ],
   "source": [
    "print(speech_texts_short[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650.5992509363296\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for transcript in speech_texts_short:\n",
    "    words = transcript.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN News - Data\n",
    "Data Source: https://huggingface.co/datasets/cnn_dailymail/viewer/3.0.0/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/bnnlukas/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8d06a55a504077af74fa873a2d2c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the News-Dataset from Huggingface\n",
    "news_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 1000 texts of the dataset\n",
    "dataset_news = news_dataset['train']\n",
    "news_texts = dataset_news['article'][:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594.7454545454545\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for text in news_texts:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "BAGHDAD, Iraq (CNN) -- Dressed in a Superman shirt, 5-year-old Youssif held his sister's hand Friday, seemingly unaware that millions of people across the world have been touched by his story. Nearby, his parents talked about the new future and hope they have for their boy -- and the potential for recovery from his severe burns. Youssif holds his sister's hand Friday. He's wearing a facial mask often used to help burn victims. It's the best birthday present the Iraqi family could ever have imagined for their boy: Youssif turns 6 next Friday. \"I was so happy I didn't know what to do with myself,\" his mother, Zainab, told CNN, a broad smile across her face. \"I didn't think the reaction would be this big.\" His father said he was on the roof of his house when CNN called him with the news about the outpouring of support for his son. \"We just want to thank everyone who has come forward,\" he said. \"We knew there was kindness out there.\" Like his wife, he couldn't stop smiling. He talked about how he tried in vain to get help for his son in Baghdad, leaving \"no stone unturned\" on a mission to help his boy. There were many trips to the Ministry of Health. He says he even put in a request to Iraq's parliament for help. The family eventually told CNN their story -- that Youssif was grabbed by masked men outside their home on January 15, doused in gasoline and set on fire. Simply by coming forward, his parents put themselves in incredible danger. No one has been arrested or held accountable in Youssif's case.  Watch CNN's Arwa Damon describe 'truly phenomenal' outpouring » . Shortly after Youssif's story aired Wednesday, the Children's Burn Foundation -- a nonprofit organization based in Sherman Oaks, California, that provides support for burn victims locally, nationally and internationally -- agreed to pay for the transportation for Youssif and his family to come to the United States and to set up a fund for donations. You can make a donation at the foundation's site by clicking here. There's a drop-down menu under the \"general donation\" area that is marked \"Youssif's fund.\" The foundation says it will cover all medical costs -- from surgeries for Youssif to housing costs to any social rehabilitation that might be needed for him. Surgeries will be performed by Dr. Peter Grossman, a plastic surgeon with the affiliated Grossman Burn Center who is donating his services for Youssif's cause. Officials are still trying to get the appropriate visas for the family's travels. \"We are prepared to have them come here, set them up in a housing situation, provide support for them and begin treatment,\" said Barbara Friedman, executive director of the Children's Burn Foundation. \"We expect that the treatment will be from between six months to a year with many surgeries.\" She added, \"He will be getting the absolute best care that's available.\" Youssif's parents said they know it's going to be a lengthy and difficult process and that adjusting to their stay in America may not be easy. But none of that matters -- getting help for their boy is first and foremost. \"I will do anything for Youssif,\" his father said, pulling his son closer to him. \"Our child is everything.\" His mother tried to coax Youssif to talk to us on this day. But he didn't want to; his mother says he's shy outside of their home. The biggest obstacle now is getting the visas to leave, and the serious security risks they face every day and hour they remain in Iraq. But this family -- which saw the very worst in humanity on that January day -- has new hope in the world. That is partly due to the tens of thousands of CNN.com users who were so moved by the story and wanted to act. CNN Iraqi staff central to bringing this story together were also overwhelmed with the generosity coming from people outside of their border. In a nation that largely feels abandoned by the rest of the world, it was a refreshing realization. E-mail to a friend . CNN.com senior producer Wayne Drash contributed to this report in Atlanta.\n"
     ]
    }
   ],
   "source": [
    "news_texts_long = []\n",
    "for text in news_texts:\n",
    "    if len(text.split()) >= 250:\n",
    "        news_texts_long.append(text)\n",
    "print(len(news_texts_long))\n",
    "print(news_texts_long[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut out the first sequuence ('BAGHDAD, Iraq (CNN) --')\n",
    "news_texts_modified = []\n",
    "for text in news_texts_long:\n",
    "    parts = text.split('--', 1)\n",
    "    if len(parts) > 1:\n",
    "        news_texts_modified.append(parts[1])  # Take the second part after the split\n",
    "    else:\n",
    "        news_texts_modified.append(text)  # Keep the original string if '--' is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dressed in a Superman shirt, 5-year-old Youssif held his sister's hand Friday, seemingly unaware that millions of people across the world have been touched by his story. Nearby, his parents talked about the new future and hope they have for their boy -- and the potential for recovery from his severe burns. Youssif holds his sister's hand Friday. He's wearing a facial mask often used to help burn victims. It's the best birthday present the Iraqi family could ever have imagined for their boy: Youssif turns 6 next Friday. \"I was so happy I didn't know what to do with myself,\" his mother, Zainab, told CNN, a broad smile across her face. \"I didn't think the reaction would be this big.\" His father said he was on the roof of his house when CNN called him with the news about the outpouring of support for his son. \"We just want to thank everyone who has come forward,\" he said. \"We knew there was kindness out there.\" Like his wife, he couldn't stop smiling. He talked about how he tried in vain to get help for his son in Baghdad, leaving \"no stone unturned\" on a mission to help his boy. There were many trips to the Ministry of Health. He says he even put in a request to Iraq's parliament for help. The family eventually told CNN their story -- that Youssif was grabbed by masked men outside their home on January 15, doused in gasoline and set on fire. Simply by coming forward, his parents put themselves in incredible danger. No one has been arrested or held accountable in Youssif's case.  Watch CNN's Arwa Damon describe 'truly phenomenal' outpouring » . Shortly after Youssif's story aired Wednesday, the Children's Burn Foundation -- a nonprofit organization based in Sherman Oaks, California, that provides support for burn victims locally, nationally and internationally -- agreed to pay for the transportation for Youssif and his family to come to the United States and to set up a fund for donations. You can make a donation at the foundation's site by clicking here. There's a drop-down menu under the \"general donation\" area that is marked \"Youssif's fund.\" The foundation says it will cover all medical costs -- from surgeries for Youssif to housing costs to any social rehabilitation that might be needed for him. Surgeries will be performed by Dr. Peter Grossman, a plastic surgeon with the affiliated Grossman Burn Center who is donating his services for Youssif's cause. Officials are still trying to get the appropriate visas for the family's travels. \"We are prepared to have them come here, set them up in a housing situation, provide support for them and begin treatment,\" said Barbara Friedman, executive director of the Children's Burn Foundation. \"We expect that the treatment will be from between six months to a year with many surgeries.\" She added, \"He will be getting the absolute best care that's available.\" Youssif's parents said they know it's going to be a lengthy and difficult process and that adjusting to their stay in America may not be easy. But none of that matters -- getting help for their boy is first and foremost. \"I will do anything for Youssif,\" his father said, pulling his son closer to him. \"Our child is everything.\" His mother tried to coax Youssif to talk to us on this day. But he didn't want to; his mother says he's shy outside of their home. The biggest obstacle now is getting the visas to leave, and the serious security risks they face every day and hour they remain in Iraq. But this family -- which saw the very worst in humanity on that January day -- has new hope in the world. That is partly due to the tens of thousands of CNN.com users who were so moved by the story and wanted to act. CNN Iraqi staff central to bringing this story together were also overwhelmed with the generosity coming from people outside of their border. In a nation that largely feels abandoned by the rest of the world, it was a refreshing realization. E-mail to a friend . CNN.com senior producer Wayne Drash contributed to this report in Atlanta.\n"
     ]
    }
   ],
   "source": [
    "print(news_texts_modified[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_class = ['News'] * len(news_texts_modified)\n",
    "\n",
    "df_news = pd.DataFrame({'Text': news_texts_modified, 'Class': news_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628.3033932135728\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for text in news_texts_modified:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jurisdictions - Data\n",
    "Data Source: https://zenodo.org/record/7151679"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n"
     ]
    }
   ],
   "source": [
    "data_path_jurisdictions = './data/Juristictions/'\n",
    "\n",
    "jurisdictions_texts = []\n",
    "for file_name in os.listdir(data_path_jurisdictions):\n",
    "    file_path = os.path.join(data_path_jurisdictions, file_name)\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        text = file.read()\n",
    "        if len(text.split()) <=3000:\n",
    "            continue\n",
    "        else:\n",
    "            jurisdictions_texts.append(text)\n",
    "print(len(jurisdictions_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14464.241025641026\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for text in jurisdictions_texts:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jurisdictions_texts_short = []\n",
    "\n",
    "for text in jurisdictions_texts:\n",
    "    target_word_count = random.randint(500, 800)\n",
    "    total_word_count = 0\n",
    "    selected_sentences = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_length = []\n",
    "    for sentence in sentences:\n",
    "        sentence_length.append(len(sentence.split()))\n",
    "\n",
    "    max_skip = (len(sentences)*int(mean(sentence_length)))-target_word_count-1000\n",
    "    words_to_skip = random.randint(1000, max_skip)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        if total_word_count + len(words) <= words_to_skip:\n",
    "            total_word_count += len(words)\n",
    "            continue\n",
    "        if total_word_count + len(words) <= target_word_count + words_to_skip:\n",
    "            selected_sentences.append(sentence)\n",
    "            total_word_count += len(words)\n",
    "        else:\n",
    "            break\n",
    "    jurisdictions_texts_short.append(' '.join(selected_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650.4089743589743\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The mere fact that the defendant is challengin...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3) Any rule of law the effect of which is tha...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was further satisfied that it would be in th...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It merely means that Parliament was not willin...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If, in its case law since the Zambrano decisio...</td>\n",
       "      <td>Jurisdiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text         Class\n",
       "0  The mere fact that the defendant is challengin...  Jurisdiction\n",
       "1  (3) Any rule of law the effect of which is tha...  Jurisdiction\n",
       "2  I was further satisfied that it would be in th...  Jurisdiction\n",
       "3  It merely means that Parliament was not willin...  Jurisdiction\n",
       "4  If, in its case law since the Zambrano decisio...  Jurisdiction"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jurisdictions_class = ['Jurisdiction'] * len(jurisdictions_texts_short)\n",
    "\n",
    "# Create the Dataframe for the jurisdictions\n",
    "df_jurisdictions = pd.DataFrame({'Text': jurisdictions_texts_short, 'Class': jurisdictions_class})\n",
    "\n",
    "array_jurisdictions = []\n",
    "for text in jurisdictions_texts_short:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array_jurisdictions.append(num_words)\n",
    "\n",
    "# Print the average number of words in each text\n",
    "print(mean(array_jurisdictions))\n",
    "\n",
    "# Print the first rows of the dataframe\n",
    "df_jurisdictions.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature - Data\n",
    "Data Source: https://github.com/pgcorpus/gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "952\n"
     ]
    }
   ],
   "source": [
    "data_path_literature = '../data/Literature/'\n",
    "literature_texts = []\n",
    "\n",
    "for file_name in os.listdir(data_path_literature):\n",
    "    file_path = os.path.join(data_path_literature, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        text = file.read()\n",
    "        if len(text.split()) <=30000:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_text = re.sub(r'\\[.*?\\]', '', text)\n",
    "            cleaned_text = re.sub(r'\\{.*?\\}', '', cleaned_text)\n",
    "            cleaned_text = cleaned_text.replace('\\n', ' ')\n",
    "            cleaned_text = cleaned_text.replace('\\\\', '')\n",
    "            # Replace all multispaces with singlespaces to get texts that are more clean\n",
    "            cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "            if 'Language: English' in cleaned_text:\n",
    "                literature_texts.append(cleaned_text)\n",
    "print(len(literature_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105257.3193277311\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for text in literature_texts:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c5/2zgnc0p131qddyd091x_x8w00000gn/T/ipykernel_36153/3561113436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_word_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mselected_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0msentence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \"\"\"\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \"\"\"\n\u001b[1;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0;31m# Find the word before the current match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             \u001b[0mbefore_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0mbefore_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "literature_texts_short = []\n",
    "\n",
    "for text in literature_texts:\n",
    "    target_word_count = random.randint(500, 800)\n",
    "    total_word_count = 0\n",
    "    selected_sentences = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentence_length = []\n",
    "    for sentence in sentences:\n",
    "        sentence_length.append(len(sentence.split()))\n",
    "\n",
    "    max_skip = (len(sentences)*int(mean(sentence_length)))-target_word_count-20000\n",
    "    words_to_skip = random.randint(5000, max_skip)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        if total_word_count + len(words) <= words_to_skip:\n",
    "            total_word_count += len(words)\n",
    "            continue\n",
    "        if total_word_count + len(words) <= target_word_count + words_to_skip:\n",
    "            selected_sentences.append(sentence)\n",
    "            total_word_count += len(words)\n",
    "        else:\n",
    "            break\n",
    "    literature_texts_short.append(' '.join(selected_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650.4012605042017\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for text in literature_texts_short:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He sat up, yawned, sneezed, shook himself, and began to rake among the burning embers of my fire with his naked hand. Presently he found the white stone, which was now red-hot—at any rate it glowed as though it were—and after examining it for a moment finally popped it into his mouth! Then he hunted in the other fire for the black stone, which he treated in a similar fashion. The next thing I remember was that the fires, which had died away almost to nothing, were burning very brightly again, I suppose because someone had put fuel on them, and Zikali was speaking. “Come here, O Macumazana and O Son of Matiwane,” he said, “and I will repeat to you what your spirits have been telling me.” We drew near into the light of the fires, which for some reason or other was extremely vivid. Then he spat the white stone from his mouth into his big hand, and I saw that now it was covered with lines and patches like a bird’s egg. “You cannot read the signs?” he said, holding it towards me; and when I shook my head went on: “Well, I can, as you white men read a book. All your history is written here, Macumazahn; but there is no need to tell you that, since you know it, as I do well enough, having learned it in other days, the days of Dingaan, Macumazahn. All your future, also, a very strange future,” and he scanned the stone with interest. “Yes, yes; a wonderful life, and a noble death far away. But of these matters you have not asked me, and therefore I may not tell them even if I wished, nor would you believe if I did. It is of your hunting trip that you have asked me, and my answer is that if you seek your own comfort you will do well not to go. A pool in a dry river-bed; a buffalo bull with the tip of one horn shattered. Yourself and the bull in the pool. Saduko, yonder, also in the pool, and a little half-bred man with a gun jumping about upon the bank. Then a litter made of boughs and you in it, and the father of Mameena walking lamely at your side. Then a hut and you in it, and the maiden called Mameena sitting at your side. “Macumazahn, your spirit has written on this stone that you should beware of Mameena, since she is more dangerous than any buffalo. If you are wise you will not go out hunting with Umbezi, although it is true that hunt will not cost you your life. There, away, Stone, and take your writings with you!” and as he spoke he jerked his arm and I heard something whiz past my face. Next he spat out the black stone and examined it in similar fashion. “Your expedition will be successful, Son of Matiwane,” he said. “Together with Macumazahn you will win many cattle at the cost of sundry lives. But for the rest—well, you did not ask me of it, did you? Also, I have told you something of that story before to-day. Away, Stone!” and the black pebble followed the white out into the surrounding gloom. We sat quite still until the dwarf broke the deep silence with one of his great laughs. “My witchcraft is done,” he said. “A poor tale, was it not? Well, hunt for those stones to-morrow and read the rest of it if you can. Why did you not ask me to tell you everything while I was about it, White Man? It would have interested you more, but now it has all gone from me back into your spirit with the stones. Saduko, get you to sleep. Macumazahn, you who are a Watcher-by-Night, come and sit with me awhile in my hut, and we will talk of other things. All this business of the stones is nothing more than a Kafir trick, is it, Macumazahn?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literature_class = ['Literature'] * len(literature_texts_short)\n",
    "\n",
    "# Create the Dataframe for the literature texts\n",
    "df_literature = pd.DataFrame({'Text': literature_texts_short, 'Class': literature_class})\n",
    "\n",
    "# Print the first rows of the dataframe\n",
    "df_literature['Text'][10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blogs - Data\n",
    "Data Source: https://www.kaggle.com/datasets/rtatman/blog-authorship-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the blog dataset\n",
    "df_blo = pd.read_csv('../data/Blogs/blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1184.917\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "# Create array with blogs over 1000 words to cut too short blogs\n",
    "large_blog_texts = []\n",
    "array = []\n",
    "for text in df_blo['text']:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    if 1000 <= num_words <= 1500 and n < 1000:\n",
    "        n+=1\n",
    "        large_blog_texts.append(text)\n",
    "        array.append(num_words)\n",
    "# Print the average number of words in each text\n",
    "print(n)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arround 650 words in each blog text\n",
    "target_word_count = 650\n",
    "\n",
    "# Skip the first 200 words to just get clear text and not the title, the authors, the copyright, etc.\n",
    "words_to_skip = 200\n",
    "blog_texts = []\n",
    "\n",
    "for text in large_blog_texts:\n",
    "    total_word_count = 0\n",
    "    selected_sentences = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "\n",
    "        if total_word_count + len(words) <= words_to_skip:\n",
    "            total_word_count += len(words)\n",
    "            continue\n",
    "        if total_word_count + len(words) <= target_word_count + words_to_skip:\n",
    "            selected_sentences.append(sentence)\n",
    "            total_word_count += len(words)\n",
    "        else:\n",
    "            break\n",
    "    if len(' '.join(selected_sentences).split()) >= 250:\n",
    "        blog_texts.append(' '.join(selected_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649.6973947895791\n"
     ]
    }
   ],
   "source": [
    "# Print the average number of words in each text\n",
    "array = []\n",
    "for text in blog_texts:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i saw one (two, actually, but one appeared to ...</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't help it HIM (4:38:07 PM): guess you do...</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Italian-Australian immigrants always harken ba...</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your whole family is a bit embarrassed, but yo...</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We each buy nothing, coming back to the realit...</td>\n",
       "      <td>Blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Class\n",
       "0  i saw one (two, actually, but one appeared to ...  Blog\n",
       "1  I can't help it HIM (4:38:07 PM): guess you do...  Blog\n",
       "2  Italian-Australian immigrants always harken ba...  Blog\n",
       "3  Your whole family is a bit embarrassed, but yo...  Blog\n",
       "4  We each buy nothing, coming back to the realit...  Blog"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_class = ['Blog'] * len(blog_texts)\n",
    "\n",
    "# Create the dataframe for the blog texts\n",
    "df_blogs = pd.DataFrame({'Text': blog_texts, 'Class': blog_class})\n",
    "\n",
    "# Print the first rows of the dataframe\n",
    "df_blogs.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have heard nothing from the Ambassador about...</td>\n",
       "      <td>Political speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I think it is in the public interest to procee...</td>\n",
       "      <td>Political speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The A-11 aircraft now at Edwards Air force Bas...</td>\n",
       "      <td>Political speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is one of the most comprehensive bills in t...</td>\n",
       "      <td>Political speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So long as there remains a man without a job, ...</td>\n",
       "      <td>Political speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text             Class\n",
       "0  I have heard nothing from the Ambassador about...  Political speech\n",
       "1  I think it is in the public interest to procee...  Political speech\n",
       "2  The A-11 aircraft now at Edwards Air force Bas...  Political speech\n",
       "3  It is one of the most comprehensive bills in t...  Political speech\n",
       "4  So long as there remains a man without a job, ...  Political speech"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the resulting dataframe for all texts with the related Classes\n",
    "result_df = pd.concat([df_speeches, df_news, df_jurisdictions, df_literature, df_blogs], ignore_index=True)\n",
    "\n",
    "# Print the first rows of the resulting dataframe \n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4533\n"
     ]
    }
   ],
   "source": [
    "print(len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in result_df.iterrows():\n",
    "    text = row['Text']\n",
    "    words = text.split()\n",
    "    if len(words) < 230:\n",
    "        result_df = result_df.drop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646.0569787985866\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "# Print the average number of words in each text in the resulting dataframe\n",
    "array = []\n",
    "for text in result_df['Text']:\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    array.append(num_words)\n",
    "print(mean(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the resulting dataframe to a csv-File in the 'data/Result/' folder\n",
    "result_df.to_csv('../data/Result/dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
